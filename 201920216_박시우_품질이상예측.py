# -*- coding: utf-8 -*-
"""201920216_박시우_품질이상예측.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17e-O9OaYmYIOdeDvq8PT2XdCEv3mTDZ1

##1. 이미지 데이터 불량 탐지

###Import Library
"""

import numpy as np
import pandas as pd
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, classification_report, precision_score, f1_score,recall_score, accuracy_score
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import seaborn as sns

"""###Load Data"""

# 2. Image DATA
data_generator = ImageDataGenerator(rescale=1./255)
# 0~255로 표현되는 픽셀 값을 0~1로 정규화
train_generator = data_generator.flow_from_directory(
    '/content/drive/MyDrive/2022-2/제조/data/resized/학습',
    target_size=(100,100),
    batch_size=1161,
    class_mode='binary'
)
test_generator = data_generator.flow_from_directory(
    '/content/drive/MyDrive/2022-2/제조/data/resized/테스트',
    target_size=(100,100),
    batch_size=291,
    class_mode='binary'
)
# 이미지 파일 저장 경로로부터 사이즈를 동일하게 하여 불량/양품 데이터 불러오기

# 지도학습을 위해 x(=이미지 픽셀 값), y(=불량/양품 구분) 지정
x_train, y_train = train_generator.next()
# 학습 데이터 할당
x_test, y_test = test_generator.next()
# 테스트 데이터 할당

# 레이블 데이터 원 핫 벡터로 만들기
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

print('x: {}\ny: {}'.format(x_train[0].shape, y_train[0].shape))
plt.imshow(x_train[0])
# 데이터가 잘 로드되었는지 확인

"""###CNN Model Learning"""

# 모델 설계
model = models.Sequential()
model.add(layers.Conv2D(32, (2,2), activation='relu', input_shape=(100,100,3)))
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Conv2D(64, (2,2), activation='relu'))
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Dropout(0.2))
model.add(layers.Conv2D(64, (2,2), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(2, activation='softmax'))
model.summary()

model.compile(optimizer=optimizers.SGD(lr=0.01), loss='binary_crossentropy',metrics=['acc'])
# 모델 컴파일

callback = EarlyStopping(monitor='val_loss', patience=5)
# early stopping 정의

learn=model.fit(x_train, y_train, batch_size=64, epochs=60, validation_split=0.2, callbacks=[callback])
# 모델 학습

"""### CNN Model Evaluation"""

plt.figure(figsize=(7,7))
plt.plot(learn.history['acc'], label='Accuracy')
plt.plot(learn.history['val_acc'], label='Val_Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
# 모델 정확도 그래프

plt.figure(figsize=(7,7))
plt.plot(learn.history['loss'], label='Loss')
plt.plot(learn.history['val_loss'], label='Val_Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# 모델 손실 그래프

pred = model.predict(x_test)
pred = np.argmax(pred, axis=1)
true = np.argmax(y_test, axis=1)
sns.heatmap(confusion_matrix(true, pred), annot=True)

print(classification_report(true, pred))

precision_score(true, pred)

recall_score(true, pred)

f1_score(true, pred)

accuracy_score(true, pred)

"""### VGGnet model learning"""

# VGG16
img_input = layers.Input(shape=(100,100,3))

def conv_block(inputs, filter=32, kernel_size=(3, 3), stride=(1, 1), padding='same', activation='relu', block=1, layer=1):
    x = layers.Conv2D(filters=filter, kernel_size=kernel_size, strides=stride, padding=padding, activation=activation , name=f'block{block}_conv{layer}')(inputs)
    return x

def VGG16():
    # Block 1
    x = conv_block(img_input, filter=32, block=1, layer=1)
    x = conv_block(x, filter=32, block=1, layer=2)
    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

    # Block 2
    x = conv_block(x, filter=32, block=2, layer=1)
    x = conv_block(x, filter=32, block=2, layer=2)
    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

    # Block 3
    x = conv_block(x, filter=64, block=3, layer=1)
    x = conv_block(x, filter=64, block=3, layer=2)
    x = conv_block(x, filter=64, block=3, layer=3)
    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

    # Block 4
    x = conv_block(x, filter=128, block=4, layer=1)
    x = conv_block(x, filter=128, block=4, layer=2)
    x = conv_block(x, filter=128, block=4, layer=3)
    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

    # Block 5
    x = conv_block(x, filter=128, block=5, layer=1)
    x = conv_block(x, filter=128, block=5, layer=2)
    x = conv_block(x, filter=128, block=5, layer=3)
    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_p128l')(x)

    # Classification block
    x = layers.Flatten(name='flatten')(x)
    x = layers.Dense(512, activation='relu', name='fc1')(x)
    x = layers.Dense(512, activation='relu', name='fc2')(x)
    x = layers.Dense(2, activation='softmax', name='predictions')(x)

    # Create model.
    model = models.Model(img_input, x, name='vgg16')
    return model

vgg16_model = VGG16()

vgg16_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',metrics=['acc'])

vgg16_model.summary()

callback = EarlyStopping(monitor='val_loss', patience=3)
# early stopping 정의
learn=vgg16_model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.2, callbacks=[callback])
# 모델 학습

plt.figure(figsize=(7,7))
plt.plot(learn.history['acc'], label='Accuracy')
plt.plot(learn.history['val_acc'], label='Val_Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
# 모델 정확도 그래프

plt.figure(figsize=(7,7))
plt.plot(learn.history['loss'], label='Loss')
plt.plot(learn.history['val_loss'], label='Val_Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# 모델 손실 그래프

pred = vgg16_model.predict(x_test)
pred = np.argmax(pred, axis=1)
true = np.argmax(y_test, axis=1)
sns.heatmap(confusion_matrix(true, pred), annot=True)

accuracy_score(true, pred), precision_score(true, pred), recall_score(true, pred), f1_score(true, pred)

"""##2. 수치 데이터 회귀 예측

###import library
"""

!pip install autokeras

import os
import pandas as pd
import numpy as np
import cv2
import matplotlib.pyplot as plt
import datetime
from sklearn import tree
import autokeras as ak
import seaborn as sns
import glob
from PIL import Image
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from math import sqrt
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve
from sklearn.metrics import classification_report
import natsort
from sklearn.tree import DecisionTreeRegressor

"""###Load data"""

root_dir = '/content/drive/MyDrive/2022-2/제조/data'
f_lists = os.listdir(root_dir)
new_file_lists = [f for f in f_lists if f.endswith('.csv')]
print("File Lists : ", new_file_lists)

ordered_list = natsort.natsorted(new_file_lists)

def csv_read_(data_dir, data_list):
    tmp = pd.read_csv(os.path.join(data_dir, data_list), sep=',', encoding='utf-8')
    y, m, d =map(int, data_list.split('-')[-1].split('.')[:-1])
    time = tmp['Time'].str[:-2]
    tmp['DTime'] ='-'.join(data_list.split('-')[-1].split('.')[:-1])
    ctime = time.apply(lambda _ : _.replace(u'오후', 'PM').replace(u'오전', 'AM'))
    n_time = ctime.apply(lambda _ : datetime.datetime.strptime(_, "%p %I:%M:%S"))
    newtime = n_time.apply(lambda _ : _.replace(year=y, month=m, day=d))
    tmp['Time'] = newtime
    return tmp

data_lists = ordered_list[1:]
error_list = ordered_list[0]

dd = csv_read_(root_dir, data_lists[0])
for i in range(1, len(data_lists)):
    dd = pd.merge(dd, csv_read_(root_dir, data_lists[i]), how='outer')

dd = dd[['Lot','Time','pH','Temp','Voltage','DTime']]

dd = dd.set_index('Time')

dd.head()

dd.describe()

dd.info()

dd.isnull().sum()
# 결측치 여부 확인 -> 없기 때문에 결측치 제거 필요 x

dd.hist(figsize=(10,10))

sns.heatmap(dd.corr(), annot=True, fmt='.2f')

# Lot list 추출
lot_lists = dd['Lot'].unique()
lot_lists

# Date list 추출
d_lists = dd['DTime'].unique()
d_lists

error = pd.read_csv(os.path.join(root_dir, error_list), sep=',', encoding='UTF8')
error.head()

# 종속변수가 모두 결측치인 행 제거
error_drop = error.dropna(thresh=2)
error_drop

# 학습에 사용되는 데이터 추출
lot_error_lists = []
lot_error_lists1 = error_drop['1'].unique().tolist()
# '1'열의 고유한 로트 값만을 가져옴
lot_error_lists2 = error_drop['2'].unique()[~np.isnan(error_drop['2'].unique())].tolist()
#'2'열의 고유한 로트 값만을 가져오며 이때 nan은 가져오지 않음
lot_error_lists = lot_error_lists1 + lot_error_lists2
# 에러가 난 로트 리스트로 취합
lot_error_lists = list(map(int, lot_error_lists))
# 리스트의 값을 정수타입으로 변환
d_error_lists = error_drop['0'].unique()
# 에러가 난 고유한 날짜만을 가져옴
print("Unique LoT List : ", lot_error_lists)
print("Unique Date List : ", d_error_lists)

#training Data 생성
X_data = pd.DataFrame(columns={'pH','Temp','Voltage','QC'})

for d in d_lists:
  #에러가 난 날짜에 속할 때
  for lot in lot_lists:
   #모든 로트에 대하여
    tmp = dd[(dd['DTime']==d)&(dd['Lot']==lot)]
    #에러가 난 날 모든 로트의 값을 가져와 저장
    tmp = tmp[['pH', 'Temp','Voltage']]
    error_df = error_drop[(error_drop['0']==d)&((error_drop['1']==lot)|(error_drop['2']==lot))]
    #에러가 난 날의
    len_error = len(error_df)
    if len_error>0:
      trr = np.full((tmp['pH'].shape), 0)
    else:
      trr = np.full((tmp['pH'].shape), 1)
    tmp['QC'] = trr
    X_data = X_data.append(tmp)
X_data = X_data.apply(pd.to_numeric)



X_data.describe() # 모든 QC값이 동일함

fig, ax = plt.subplots(figsize=(7,7))
sns.heatmap(X_data.corr(),
 cmap='RdYlBu_r',
 annot = True,
 linewidths=0.5,
 cbar_kws={"shrink":.5},
 vmin=-1, vmax=1)
plt.show()

train_data, test_data = train_test_split(X_data, test_size=0.2)

"""###decision Tree model learning"""

model = DecisionTreeRegressor(max_depth=5)

model = model.fit(X_data[['pH','Temp','Voltage']],X_data[['QC']])

vis = True
if vis:
 plt.figure(figsize=(10,10))
 tree.plot_tree(model)
 plt.show()

"""###Model Evaluation"""

model_predicted = model.predict(test_data[['pH','Temp','Voltage']])
print('Decision Tree Model Predict : ', model_predicted)
rmse = sqrt(mean_squared_error(test_data['QC'], model_predicted))
print('Decision Tree Model RMSE : ',rmse)

y_test = test_data['QC']
y_pred = [round(y, 0) for y in model_predicted]
print("accuracy = ", accuracy_score(y_test, y_pred))
print("recall = ", recall_score(y_test, y_pred))
print("precision = ", precision_score(y_test, y_pred))
print("f1 score = ", f1_score(y_test, y_pred))

sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='.2f')

print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))

def get_confusion_matrix_values(y_true, y_pred):
 cm = confusion_matrix(y_true, y_pred)
 return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])
TP, FP, FN, TN = get_confusion_matrix_values(y_test, y_pred)
print("TP : ", TP)
print("FP : ", FP)
print("FN : ", FN)
print("TN : ", TN)
if (TP+FN) == 0:
 tpr_val = 0
else:
 tpr_val = TP / (TP+FN)
if (TN+FP) == 0:
 fpr_val = 0
else:
 fpr_val = TN / (TN+FP)
print(tpr_val, fpr_val)
tpr, fpr, _ = roc_curve(y_test, y_pred)
tpr[1] = tpr_val
fpr[1] = fpr_val
if len(tpr) < 3:
 tpr = np.append(tpr, 1)
 fpr = np.append(fpr, 1)
print(fpr, tpr)

plt.plot(tpr, fpr, 'o-', label="Logistic Regression")
plt.plot([0, 1], [0, 1], 'k--', label="random guess")
plt.plot([tpr_val], [fpr_val], 'ro', ms=10)
plt.xlabel('Fall-Out')
plt.ylabel('Recall')
plt.title('Receiver operating characteristic example')
plt.grid()
plt.legend()
plt.show()

fig = plt.figure(figsize=(30, 30))
ax1 = fig.add_subplot(2,2,1, projection='3d')
ax1.scatter(test_data['pH'], test_data['Temp'], model_predicted)
ax1.set_xlabel('pH Test Data')
ax1.set_ylabel('Temp Test Data')
ax1.set_zlabel('Estimated Process Data')
ax2 = fig.add_subplot(2,2,2, projection='3d')
ax2.scatter(test_data['Temp'], test_data['Voltage'], model_predicted)
ax2.set_xlabel('Temp Test Data')
ax2.set_ylabel('Voltage Test Data')
ax2.set_zlabel('Estimated Process Data')
ax3 = fig.add_subplot(2,2,3, projection='3d')
ax3.scatter(test_data['pH'], test_data['Voltage'], model_predicted)
ax3.set_xlabel('pH Test Data')
ax3.set_ylabel('Voltage Test Data')
ax3.set_zlabel('Estimated Process Data')
plt.tight_layout()
plt.show()

